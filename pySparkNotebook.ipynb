{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SENG 550 PROJECT: AMAZON REVIEW CLASSIFIER"
      ],
      "metadata": {
        "id": "tAgBjr6cc3gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First the Setup Spark in Google Colab (Based on the notebook given in class)\n",
        "*reference: https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/*\n",
        "\n",
        "\n",
        "*to install other versions, get the download link from https://spark.apache.org/downloads.html*"
      ],
      "metadata": {
        "id": "7jb5rVb7k4W4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDm2hk6afyXH"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "XPLjf-tQgJ2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "nHDWo_1Kh9zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "QF5q4lWYj_Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "M9_wQwdxkEp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "_GwoL0pbkf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "id": "sZu4rGOEkkOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "To9reEGekk-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "15-xw65skr_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just testing if spark is working, for a start!"
      ],
      "metadata": {
        "id": "7Ugb1nlvefYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = sc.parallelize([1, 2, 3, 4, 5])\n",
        "test.map(lambda x: (x, x**2)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPwANqTjkwEO",
        "outputId": "88c32759-b247-4e33-c912-60aec4e7daaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next getting the data (the reviews json in this case), reading it and extracting useful info out of it. We use the cell phone review data in this case"
      ],
      "metadata": {
        "id": "oww-2BBVizXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/reviews_Cell_Phones_and_Accessories_5.json.gz\n",
        "!wget --no-check-certificate http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz"
      ],
      "metadata": {
        "id": "MB54Kwx9eygh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the file and extracting the useful info"
      ],
      "metadata": {
        "id": "p1LKtk9FjxYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_data_rdd = spark.read.json(\"/content/reviews_Cell_Phones_and_Accessories_5.json.gz\").rdd"
      ],
      "metadata": {
        "id": "bw0ark1okcBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how the data looks\n",
        "file_data_rdd.take(1)"
      ],
      "metadata": {
        "id": "p8jn2vdIlBUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For our binary classifier (good or bad review), only the fields the that are useful are:\n",
        "# overall, reviewText, and possibly helpful so we'll extract those\n",
        "from pyspark.sql import Row\n",
        "review_data_rdd = file_data_rdd.map(lambda row: Row(rating=row.overall, helpfulness=row.helpful, text=row.reviewText ))"
      ],
      "metadata": {
        "id": "cPwJyg5dlM8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data_rdd.take(1)"
      ],
      "metadata": {
        "id": "s5VEf8HdnkRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we are done with extracting the data, let us do a bit of cleaning to make sure we get consistent results and also do a bit of exploratory data analysis"
      ],
      "metadata": {
        "id": "SIeS8mD3xUGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coming to cleaning part\n",
        "# For each review, we do need all the three\n",
        "# So we will filter out the reviews that don't have those\n",
        "\n",
        "cleaned_rdd = review_data_rdd.filter(lambda row: row.rating is not None and row.helpfulness is not None and row.text is not None)\n",
        "\n",
        "\n",
        "\n",
        "# People are generous and they usually give high ratings (sometimes more than what they should)\n",
        "# So We'll define the threshold for a favourable or good review to be 3.5 \n",
        "# Anything lower than that is bad\n",
        "\n",
        "GOOD_REVIEW_THRESHOLD = 3.5\n",
        "\n",
        "number_of_total_reviews = cleaned_rdd.count()\n",
        "number_of_good_reviews = cleaned_rdd.filter(lambda row: row.rating >=3.5).count()"
      ],
      "metadata": {
        "id": "QyTaFmcqx4nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Reviews: \", number_of_total_reviews)\n",
        "print(\"Good Reviews: \", number_of_good_reviews)\n",
        "print(\"Bad Reviews: \", number_of_total_reviews - number_of_good_reviews)"
      ],
      "metadata": {
        "id": "NE-qShrBzUdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As Evident from the data that we have, the data is heavily skewed towards good reviews i.e. a review would be a good review. As mentioned in the class, in cases like these, it is not best practise to evaluate the model on the basis of its accuracy, we would need measures such as recall, precision and F-score to test the certainity the certainity of our model actually working and doing its job well."
      ],
      "metadata": {
        "id": "dlcJvvG20jS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now coming to the part of simplyfying the RDD ,splitting it into datasets, extracting features and building on top of them\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I2_ug9hg4QCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now\n",
        "\n",
        "\n",
        "# Helpfulness is an list in our RDD, we can possibly change that to be an average\n",
        "# As it would be an easier metric to work with\n",
        "# And this would be the final (the original dataset)\n",
        "# Before we end up splitting it up\n",
        "\n",
        "# Also rating on its own doesn't \n",
        "# We'll use the rating to tag a review with label 'good' or 'bad' to aid in training afterwards\n",
        "# For us Good Review = 1 and Bad Review = 0\n",
        "\n",
        "original_rdd = cleaned_rdd.map(lambda row: Row(helpful_rating=sum(row.helpfulness)/len(row.helpfulness), helpful_no= len(row.helpfulness), text = row.text, label = 1 if row.rating >= 3.5 else 0))\n",
        "original_rdd.take(3)"
      ],
      "metadata": {
        "id": "fTMxJLTi4b2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now it's time to use the original_rdd to extract features and move forward in the process of developing a model based on that"
      ],
      "metadata": {
        "id": "k9vITlKqDC2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From here on we make use of the training rdd \n",
        "# For coming up with our model\n",
        "\n",
        "# Importing the required libraries for RegexTokenizer, StopWordsRemover, HashingTF\n",
        "from pyspark.ml import *\n",
        "from pyspark.ml.feature import *\n",
        "\n",
        "\n",
        "# Converting the RDD to a Database as it is easier to work with (a dataframe is easier to work with column-based data) \n",
        "# A Data Frame gives us the same scalability capabilities so why not use it\n",
        "original_df = original_rdd.toDF()\n",
        " \n",
        "# Developing the tokenizer (to be used for tokenizing each word of the review text)\n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\W\")\n",
        "df_tokenized = tokenizer.transform(original_df)\n",
        "\n",
        "# To remove all the common words to be used in a sentence (which are of no use to our model)\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "df_filtered = remover.transform(df_tokenized)\n",
        "\n",
        "# Converting the filtered words into numerical raw features\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"features\")\n",
        "df_features_raw = hashingTF.transform(df_filtered)\n",
        "\n",
        "# Let's see how the different Transformation fields look like\n",
        "df_features_raw.take(1)"
      ],
      "metadata": {
        "id": "SdwgJUaF60kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it is time to extract only those feature columns that we actually do need to train the model\n",
        "df_features = df_features_raw.select('features', 'helpful_rating', 'helpful_no', 'label')\n",
        "\n",
        "# This how they look\n",
        "df_features.take(1)"
      ],
      "metadata": {
        "id": "jfamDXme_RbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the feature dataset into different datasets - training, test and cross-validation data"
      ],
      "metadata": {
        "id": "MTv4nn3sBuR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# So we have original data set pretty much set\n",
        "# It is time to split it up into training, test and cross validation data sets\n",
        "\n",
        "df_training, df_test, df_cross_validation = df_features.randomSplit([0.5, 0.2, 0.3])"
      ],
      "metadata": {
        "id": "V8lf6IYc6KCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now training the model on training data"
      ],
      "metadata": {
        "id": "1kUKaL5-CKh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(labelCol='label')\n",
        "model = lr.fit(df_training)\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "\n",
        "predictions_df = model.transform(df_cross_validation)\n",
        "\n",
        "# Evaluate model performance\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "print(evaluator.evaluate(predictions_df))\n"
      ],
      "metadata": {
        "id": "NoaLWItAJ8DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking ideal max iterations\n",
        "After some tests we noticed that `maxIter=10` is a sweet spot to mantain model complexity. After having done several technical and statistical analysis when the iterations goes beyond the value of `10` the model complexity is probably too high. This implies data is **overfitting** to a high extent.\n",
        "Therefore, we limited to not do more than `10` iterations to prevent model complexity to go too high. "
      ],
      "metadata": {
        "id": "221WsnVBdhQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(labelCol='label')\n",
        "lr.setMaxIter(10)\n",
        "model = lr.fit(df_training)\n",
        "\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "test_predictions_df = model.transform(df_training)\n",
        "\n",
        "# Evaluate model performance\n",
        "test_predictions_df.groupBy('label', 'prediction').count().show()\n",
        "TN = test_predictions_df.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = test_predictions_df.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = test_predictions_df.filter('prediction = 0 AND label <> prediction').count()\n",
        "FP = test_predictions_df.filter('prediction = 1 AND label <> prediction').count()\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "F =  2 * (precision*recall) / (precision + recall)\n",
        "print(\"testing data\")\n",
        "print('\\t',precision)\n",
        "print('\\t',recall)\n",
        "print('\\t',accuracy)\n",
        "print('\\t',F)\n",
        "\n",
        "print()\n",
        "print()\n",
        "# CROSS VALIDATE\n",
        "\n",
        "predictions_df = model.transform(df_cross_validation)\n",
        "\n",
        "# Evaluate model performance\n",
        "predictions_df.groupBy('label', 'prediction').count().show()\n",
        "TN = predictions_df.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = predictions_df.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = predictions_df.filter('prediction = 0 AND label <> prediction').count()\n",
        "FP = predictions_df.filter('prediction = 1 AND label <> prediction').count()\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "F =  2 * (precision*recall) / (precision + recall)\n",
        "print(\"cross validation\")\n",
        "print('\\t', precision)\n",
        "print('\\t', recall)\n",
        "print('\\t', accuracy)\n",
        "print('\\t', F)"
      ],
      "metadata": {
        "id": "QTX86keJamKB",
        "outputId": "587d738e-f1f1-4f44-8a83-7f0974754243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 2172|\n",
            "|    0|       1.0| 4036|\n",
            "|    0|       0.0|18677|\n",
            "|    1|       1.0|72632|\n",
            "+-----+----------+-----+\n",
            "\n",
            "testing data\n",
            "\t 0.9473574372619606\n",
            "\t 0.9709641195657986\n",
            "\t 0.9363393049417025\n",
            "\t 0.9590155276222669\n",
            "\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 4894|\n",
            "|    0|       1.0| 5231|\n",
            "|    0|       0.0| 8476|\n",
            "|    1|       1.0|39449|\n",
            "+-----+----------+-----+\n",
            "\n",
            "cross validation\n",
            "\t 0.8829230080572963\n",
            "\t 0.8896330875222696\n",
            "\t 0.8255813953488372\n",
            "\t 0.8862653471574762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(labelCol='label')\n",
        "lr.setMaxIter(20)\n",
        "model = lr.fit(df_training)\n",
        "\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "test_predictions_df = model.transform(df_training)\n",
        "\n",
        "# Evaluate model performance\n",
        "test_predictions_df.groupBy('label', 'prediction').count().show()\n",
        "TN = test_predictions_df.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = test_predictions_df.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = test_predictions_df.filter('prediction = 0 AND label <> prediction').count()\n",
        "FP = test_predictions_df.filter('prediction = 1 AND label <> prediction').count()\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "F =  2 * (precision*recall) / (precision + recall)\n",
        "print(\"testing data\")\n",
        "print('\\t',precision)\n",
        "print('\\t',recall)\n",
        "print('\\t',accuracy)\n",
        "print('\\t',F)\n",
        "\n",
        "print()\n",
        "print()\n",
        "# CROSS VALIDATE\n",
        "\n",
        "predictions_df = model.transform(df_cross_validation)\n",
        "\n",
        "# Evaluate model performance\n",
        "predictions_df.groupBy('label', 'prediction').count().show()\n",
        "TN = predictions_df.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = predictions_df.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = predictions_df.filter('prediction = 0 AND label <> prediction').count()\n",
        "FP = predictions_df.filter('prediction = 1 AND label <> prediction').count()\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "F =  2 * (precision*recall) / (precision + recall)\n",
        "print(\"cross validation\")\n",
        "print('\\t', precision)\n",
        "print('\\t', recall)\n",
        "print('\\t', accuracy)\n",
        "print('\\t', F)"
      ],
      "metadata": {
        "id": "vkho35p0foaX",
        "outputId": "722bad8e-6e6f-40b8-9526-e5494810a153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1901|\n",
            "|    0|       1.0| 3380|\n",
            "|    0|       0.0|19333|\n",
            "|    1|       1.0|72903|\n",
            "+-----+----------+-----+\n",
            "\n",
            "testing data\n",
            "\t 0.9556913073686142\n",
            "\t 0.9745869204855355\n",
            "\t 0.945845339786909\n",
            "\t 0.9650466287635601\n",
            "\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 5540|\n",
            "|    0|       1.0| 5594|\n",
            "|    0|       0.0| 8113|\n",
            "|    1|       1.0|38803|\n",
            "+-----+----------+-----+\n",
            "\n",
            "cross validation\n",
            "\t 0.8740004955289772\n",
            "\t 0.8750648354869991\n",
            "\t 0.8081998277347114\n",
            "\t 0.8745323416723011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing effectiveness of various models\n",
        "To improve our model we relied on 2 important key factors:\n",
        "1. number of max iterations of training: the larger it is the better it fits the data but the risk is overfitting\n",
        "2. value of regularization parameter: to allow make the model simpler and lower risk of overfitting. However too high value may yield to underfitting since model may be too simple\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c0EbM6G7btqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "regParams = np.arange(0, .1, .01)\n",
        "elasticNetParams = [0,1]\n",
        "maxIters = [10]\n",
        "\n",
        "results = []\n",
        "\n",
        "for rp in regParams:\n",
        "  for enp in elasticNetParams:\n",
        "    for mi in maxIters:\n",
        "\n",
        "      lr = LogisticRegression(labelCol='label')\n",
        "      lr.setMaxIter(mi)\n",
        "      lr.setRegParam(rp)\n",
        "      lr.setElasticNetParam(enp)\n",
        "      model = lr.fit(df_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      predictions_df = model.transform(df_cross_validation)\n",
        "\n",
        "      # Evaluate model performance\n",
        "      evaluator = BinaryClassificationEvaluator()\n",
        "      predictions_df.groupBy('label', 'prediction').count().show()\n",
        "      TN = predictions_df.filter('prediction = 0 AND label = prediction').count()\n",
        "      TP = predictions_df.filter('prediction = 1 AND label = prediction').count()\n",
        "      FN = predictions_df.filter('prediction = 0 AND label <> prediction').count()\n",
        "      FP = predictions_df.filter('prediction = 1 AND label <> prediction').count()\n",
        "      accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "      precision = TP / (TP + FP)\n",
        "      recall = TP / (TP + FN)\n",
        "      F =  2 * (precision*recall) / (precision + recall)\n",
        "      \n",
        "      key = [mi, rp , enp]\n",
        "      value = [precision, recall, accuracy, F]\n",
        "      \n",
        "      results.append([key, value])\n",
        "\n",
        "      print(\"=================================================\")\n",
        "      print(key)\n",
        "      print(\"\\t\\t\",precision)\n",
        "      print(\"\\t\\t\",recall)\n",
        "      print(\"\\t\\t\",accuracy)\n",
        "      print(\"\\t\\t\",F)"
      ],
      "metadata": {
        "id": "I6rAFASUupxs",
        "outputId": "531f5ce6-8b96-4b9a-9951-214cddd7ac20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 5009|\n",
            "|    0|       1.0| 5395|\n",
            "|    0|       0.0| 8413|\n",
            "|    1|       1.0|39566|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.0, 0]\n",
            "\t\t 0.8800071172794199\n",
            "\t\t 0.887627593942793\n",
            "\t\t 0.821797441035918\n",
            "\t\t 0.883800929235168\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 5009|\n",
            "|    0|       1.0| 5395|\n",
            "|    0|       0.0| 8413|\n",
            "|    1|       1.0|39566|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.0, 1]\n",
            "\t\t 0.8800071172794199\n",
            "\t\t 0.887627593942793\n",
            "\t\t 0.821797441035918\n",
            "\t\t 0.883800929235168\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 3371|\n",
            "|    0|       1.0| 6153|\n",
            "|    0|       0.0| 7655|\n",
            "|    1|       1.0|41204|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.01, 0]\n",
            "\t\t 0.8700720062503959\n",
            "\t\t 0.9243746494671902\n",
            "\t\t 0.8368703218402617\n",
            "\t\t 0.8964016882043249\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0|  652|\n",
            "|    0|       1.0|10946|\n",
            "|    0|       0.0| 2862|\n",
            "|    1|       1.0|43923|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.01, 1]\n",
            "\t\t 0.8005066613206\n",
            "\t\t 0.9853729669097028\n",
            "\t\t 0.8013462823082061\n",
            "\t\t 0.8833715457946181\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 2783|\n",
            "|    0|       1.0| 6686|\n",
            "|    0|       0.0| 7122|\n",
            "|    1|       1.0|41792|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.02, 0]\n",
            "\t\t 0.8620817690498783\n",
            "\t\t 0.9375659001682557\n",
            "\t\t 0.8378123768905332\n",
            "\t\t 0.8982407875081942\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0|  161|\n",
            "|    0|       1.0|12845|\n",
            "|    0|       0.0|  963|\n",
            "|    1|       1.0|44414|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.02, 1]\n",
            "\t\t 0.7756684538675143\n",
            "\t\t 0.9963881099270891\n",
            "\t\t 0.7772296730212562\n",
            "\t\t 0.8722823418504624\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 2459|\n",
            "|    0|       1.0| 6989|\n",
            "|    0|       0.0| 6819|\n",
            "|    1|       1.0|42116|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.03, 0]\n",
            "\t\t 0.8576723347927909\n",
            "\t\t 0.9448345485137409\n",
            "\t\t 0.8381720706370005\n",
            "\t\t 0.8991460290350128\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0|   15|\n",
            "|    0|       1.0|13693|\n",
            "|    0|       0.0|  115|\n",
            "|    1|       1.0|44560|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.03, 1]\n",
            "\t\t 0.7649391447650765\n",
            "\t\t 0.9996634885025238\n",
            "\t\t 0.7652056249250638\n",
            "\t\t 0.8666900066129849\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 2184|\n",
            "|    0|       1.0| 7374|\n",
            "|    0|       0.0| 6434|\n",
            "|    1|       1.0|42391|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.04, 0]\n",
            "\t\t 0.8518235707826786\n",
            "\t\t 0.9510039259674705\n",
            "\t\t 0.8362879605364575\n",
            "\t\t 0.8986856052575789\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       1.0|13806|\n",
            "|    0|       0.0|    2|\n",
            "|    1|       1.0|44575|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.04, 1]\n",
            "\t\t 0.7635189530840513\n",
            "\t\t 1.0\n",
            "\t\t 0.7635270541082164\n",
            "\t\t 0.8659038812696687\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1980|\n",
            "|    0|       1.0| 7662|\n",
            "|    0|       0.0| 6146|\n",
            "|    1|       1.0|42595|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.05, 0]\n",
            "\t\t 0.8475436257635752\n",
            "\t\t 0.9555804823331464\n",
            "\t\t 0.8348491855505884\n",
            "\t\t 0.8983254597604184\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       1.0|13808|\n",
            "|    1|       1.0|44575|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.05, 1]\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 1.0\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 0.8658870607432156\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1826|\n",
            "|    0|       1.0| 7904|\n",
            "|    0|       0.0| 5904|\n",
            "|    1|       1.0|42749|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.06, 0]\n",
            "\t\t 0.8439579096993268\n",
            "\t\t 0.959035333707235\n",
            "\t\t 0.833341897470154\n",
            "\t\t 0.8978241693619523\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       1.0|13808|\n",
            "|    1|       1.0|44575|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.06, 1]\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 1.0\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 0.8658870607432156\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1704|\n",
            "|    0|       1.0| 8122|\n",
            "|    0|       0.0| 5686|\n",
            "|    1|       1.0|42871|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.07, 0]\n",
            "\t\t 0.84072323652266\n",
            "\t\t 0.9617722938867078\n",
            "\t\t 0.8316975832005892\n",
            "\t\t 0.8971831575422735\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       1.0|13808|\n",
            "|    1|       1.0|44575|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.07, 1]\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 1.0\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 0.8658870607432156\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1595|\n",
            "|    0|       1.0| 8323|\n",
            "|    0|       0.0| 5485|\n",
            "|    1|       1.0|42980|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.08, 0]\n",
            "\t\t 0.837767771865193\n",
            "\t\t 0.9642176107683679\n",
            "\t\t 0.8301217820255896\n",
            "\t\t 0.8965560399674585\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       1.0|13808|\n",
            "|    1|       1.0|44575|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.08, 1]\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 1.0\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 0.8658870607432156\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1500|\n",
            "|    0|       1.0| 8516|\n",
            "|    0|       0.0| 5292|\n",
            "|    1|       1.0|43075|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.09, 0]\n",
            "\t\t 0.8349324494582389\n",
            "\t\t 0.9663488502523836\n",
            "\t\t 0.8284432112087423\n",
            "\t\t 0.8958467649689079\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    0|       1.0|13808|\n",
            "|    1|       1.0|44575|\n",
            "+-----+----------+-----+\n",
            "\n",
            "=================================================\n",
            "[10, 0.09, 1]\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 1.0\n",
            "\t\t 0.7634927975609338\n",
            "\t\t 0.8658870607432156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in results:\n",
        "  mi, rp, enp = key\n",
        "  precision, recall, accuracy, fscore = value\n",
        "\n",
        "  print(f\"regParam: {rp} elasticNetValue: {enp} maxIteration: {mi}\")\n",
        "  print(f\"\\tprecision: {precision}\")\n",
        "  print(f\"\\trecall: {recall}\")\n",
        "  print(f\"\\taccuracy: {accuracy}\")\n",
        "  print(f\"\\tfscore: {fscore}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "cdFGneD2w7zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3679badb-f85e-45fc-e916-f26da36da76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "regParam: 0.0 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8800071172794199\n",
            "\trecall: 0.887627593942793\n",
            "\taccuracy: 0.821797441035918\n",
            "\tfscore: 0.883800929235168\n",
            "\n",
            "regParam: 0.0 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.8800071172794199\n",
            "\trecall: 0.887627593942793\n",
            "\taccuracy: 0.821797441035918\n",
            "\tfscore: 0.883800929235168\n",
            "\n",
            "regParam: 0.01 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8700720062503959\n",
            "\trecall: 0.9243746494671902\n",
            "\taccuracy: 0.8368703218402617\n",
            "\tfscore: 0.8964016882043249\n",
            "\n",
            "regParam: 0.01 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.8005066613206\n",
            "\trecall: 0.9853729669097028\n",
            "\taccuracy: 0.8013462823082061\n",
            "\tfscore: 0.8833715457946181\n",
            "\n",
            "regParam: 0.02 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8620817690498783\n",
            "\trecall: 0.9375659001682557\n",
            "\taccuracy: 0.8378123768905332\n",
            "\tfscore: 0.8982407875081942\n",
            "\n",
            "regParam: 0.02 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7756684538675143\n",
            "\trecall: 0.9963881099270891\n",
            "\taccuracy: 0.7772296730212562\n",
            "\tfscore: 0.8722823418504624\n",
            "\n",
            "regParam: 0.03 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8576723347927909\n",
            "\trecall: 0.9448345485137409\n",
            "\taccuracy: 0.8381720706370005\n",
            "\tfscore: 0.8991460290350128\n",
            "\n",
            "regParam: 0.03 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7649391447650765\n",
            "\trecall: 0.9996634885025238\n",
            "\taccuracy: 0.7652056249250638\n",
            "\tfscore: 0.8666900066129849\n",
            "\n",
            "regParam: 0.04 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8518235707826786\n",
            "\trecall: 0.9510039259674705\n",
            "\taccuracy: 0.8362879605364575\n",
            "\tfscore: 0.8986856052575789\n",
            "\n",
            "regParam: 0.04 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7635189530840513\n",
            "\trecall: 1.0\n",
            "\taccuracy: 0.7635270541082164\n",
            "\tfscore: 0.8659038812696687\n",
            "\n",
            "regParam: 0.05 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8475436257635752\n",
            "\trecall: 0.9555804823331464\n",
            "\taccuracy: 0.8348491855505884\n",
            "\tfscore: 0.8983254597604184\n",
            "\n",
            "regParam: 0.05 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7634927975609338\n",
            "\trecall: 1.0\n",
            "\taccuracy: 0.7634927975609338\n",
            "\tfscore: 0.8658870607432156\n",
            "\n",
            "regParam: 0.06 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8439579096993268\n",
            "\trecall: 0.959035333707235\n",
            "\taccuracy: 0.833341897470154\n",
            "\tfscore: 0.8978241693619523\n",
            "\n",
            "regParam: 0.06 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7634927975609338\n",
            "\trecall: 1.0\n",
            "\taccuracy: 0.7634927975609338\n",
            "\tfscore: 0.8658870607432156\n",
            "\n",
            "regParam: 0.07 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.84072323652266\n",
            "\trecall: 0.9617722938867078\n",
            "\taccuracy: 0.8316975832005892\n",
            "\tfscore: 0.8971831575422735\n",
            "\n",
            "regParam: 0.07 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7634927975609338\n",
            "\trecall: 1.0\n",
            "\taccuracy: 0.7634927975609338\n",
            "\tfscore: 0.8658870607432156\n",
            "\n",
            "regParam: 0.08 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.837767771865193\n",
            "\trecall: 0.9642176107683679\n",
            "\taccuracy: 0.8301217820255896\n",
            "\tfscore: 0.8965560399674585\n",
            "\n",
            "regParam: 0.08 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7634927975609338\n",
            "\trecall: 1.0\n",
            "\taccuracy: 0.7634927975609338\n",
            "\tfscore: 0.8658870607432156\n",
            "\n",
            "regParam: 0.09 elasticNetValue: 0 maxIteration: 10\n",
            "\tprecision: 0.8349324494582389\n",
            "\trecall: 0.9663488502523836\n",
            "\taccuracy: 0.8284432112087423\n",
            "\tfscore: 0.8958467649689079\n",
            "\n",
            "regParam: 0.09 elasticNetValue: 1 maxIteration: 10\n",
            "\tprecision: 0.7634927975609338\n",
            "\trecall: 1.0\n",
            "\taccuracy: 0.7634927975609338\n",
            "\tfscore: 0.8658870607432156\n",
            "\n"
          ]
        }
      ]
    }
  ]
}